//! Entropy Bot — idle-time file organization analysis daemon.

use crate::entropy_scorers;
use crate::file_index::FileIndexEntry;
use anyhow::Result;
use chrono::Utc;
use rusqlite::{params, Connection};
use serde::{Deserialize, Serialize};
use std::path::{Path, PathBuf};
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::{Arc, Mutex};
use std::time::Duration;

/// Configuration for the entropy bot.
#[derive(Debug, Clone)]
pub struct EntropyBotConfig {
    /// Minutes of inactivity before running a scoring pass.
    pub idle_threshold_minutes: u64,
    /// Minimum interval between scoring passes.
    pub min_interval_minutes: u64,
    /// Maximum directories to score per pass.
    pub dirs_per_pass: usize,
}

impl Default for EntropyBotConfig {
    fn default() -> Self {
        Self {
            idle_threshold_minutes: 5,
            min_interval_minutes: 60,
            dirs_per_pass: 500,
        }
    }
}

/// Current status of the entropy bot.
#[derive(Debug, Clone, Copy, Default, PartialEq, Eq, Serialize, Deserialize)]
pub enum EntropyBotStatus {
    Idle,
    Scanning,
    Sleeping,
    #[default]
    Disabled,
}

/// A suggestion generated by the entropy bot.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Suggestion {
    pub id: i64,
    pub suggestion_type: String,
    pub affected_paths: Vec<String>,
    pub reason: String,
    pub confidence: f64,
    pub space_savings_bytes: Option<i64>,
    pub status: String,
}

/// Statistics from a scoring pass.
#[derive(Debug, Clone, Default)]
pub struct ScoringStats {
    pub dirs_scored: usize,
    pub files_scored: usize,
    pub suggestions_generated: usize,
}

/// Summary for the UI.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct EntropySummary {
    pub pending_suggestions: usize,
    pub space_reclaimable_bytes: i64,
    pub highest_entropy_dirs: Vec<(String, f64)>,
}

/// The entropy bot engine.
pub struct EntropyBot {
    conn: Arc<Mutex<Connection>>,
    config: EntropyBotConfig,
    cancel: Arc<AtomicBool>,
    staging_root: PathBuf,
}

impl EntropyBot {
    pub fn new(
        conn: Arc<Mutex<Connection>>,
        config: EntropyBotConfig,
        cancel: Arc<AtomicBool>,
    ) -> Self {
        let staging_root = dirs_home().join(".ylh-staging");
        Self {
            conn,
            config,
            cancel,
            staging_root,
        }
    }

    /// Run a single scoring pass over directories in the file index.
    pub fn score_pass(&self) -> Result<ScoringStats> {
        let conn = self.conn.lock().unwrap();
        let mut stats = ScoringStats::default();

        // Get distinct parent directories from indexed files
        let mut stmt = conn.prepare(
            "SELECT DISTINCT
                substr(path, 1, length(path) - length(name) - 1) as dir_path
             FROM files
             LIMIT ?1",
        )?;

        let dirs: Vec<String> = stmt
            .query_map(params![self.config.dirs_per_pass as i64], |row| row.get(0))?
            .filter_map(|r| r.ok())
            .collect();

        for dir in &dirs {
            if self.cancel.load(Ordering::Relaxed) {
                break;
            }

            // Get files in this directory
            let mut file_stmt = conn.prepare(
                "SELECT id, path, name, extension, size_bytes, modified_at, drive_id, indexed_at
                 FROM files
                 WHERE path LIKE ?1 || '/%'
                 AND path NOT LIKE ?1 || '/%/%'",
            )?;

            let files: Vec<FileIndexEntry> = file_stmt
                .query_map(params![dir], |row| {
                    let modified_ts: i64 = row.get(5)?;
                    let indexed_ts: i64 = row.get(7)?;
                    Ok(FileIndexEntry {
                        id: row.get(0)?,
                        path: row.get(1)?,
                        name: row.get(2)?,
                        extension: row.get(3)?,
                        size_bytes: row.get(4)?,
                        modified_at: chrono::DateTime::from_timestamp(modified_ts, 0)
                            .unwrap_or_default(),
                        drive_id: row.get(6)?,
                        indexed_at: chrono::DateTime::from_timestamp(indexed_ts, 0)
                            .unwrap_or_default(),
                    })
                })?
                .filter_map(|r| r.ok())
                .collect();

            if files.is_empty() {
                continue;
            }

            let naming = entropy_scorers::naming_entropy(&files);
            let age = entropy_scorers::age_spread(&files);
            let depth = entropy_scorers::depth_waste(Path::new(dir));

            // Duplicate ratio: count files with matching hashes
            let duplicate_ratio = {
                let file_ids: Vec<i64> = files.iter().map(|f| f.id).collect();
                if file_ids.is_empty() {
                    0.0
                } else {
                    let placeholders: Vec<String> = file_ids.iter().map(|_| "?".to_string()).collect();
                    let query = format!(
                        "SELECT COUNT(DISTINCT h1.file_id) FROM file_content_hashes h1
                         JOIN file_content_hashes h2 ON h1.sha256 = h2.sha256 AND h1.file_id != h2.file_id
                         WHERE h1.file_id IN ({})",
                        placeholders.join(",")
                    );
                    let mut dup_stmt = conn.prepare(&query)?;
                    let dup_count: i64 = dup_stmt
                        .query_row(
                            rusqlite::params_from_iter(file_ids.iter()),
                            |row| row.get(0),
                        )
                        .unwrap_or(0);
                    dup_count as f64 / files.len() as f64
                }
            };

            // Orphan score: count files with zero edges
            let orphan_score = {
                let file_ids: Vec<i64> = files.iter().map(|f| f.id).collect();
                if file_ids.is_empty() {
                    0.0
                } else {
                    let query = format!(
                        "SELECT COUNT(*) FROM ({}) t WHERE t.id NOT IN (
                            SELECT source_id FROM file_edges UNION SELECT target_id FROM file_edges
                        )",
                        file_ids.iter().map(|id| format!("SELECT {} as id", id)).collect::<Vec<_>>().join(" UNION ALL ")
                    );
                    let mut orph_stmt = conn.prepare(&query)?;
                    let orphan_count: i64 = orph_stmt
                        .query_row([], |row| row.get(0))
                        .unwrap_or(0);
                    orphan_count as f64 / files.len() as f64
                }
            };

            let composite = entropy_scorers::composite_score(
                naming,
                age,
                depth,
                duplicate_ratio,
                orphan_score,
            );

            // Upsert scores for each file in this directory
            let now = Utc::now().to_rfc3339();
            for f in &files {
                conn.execute(
                    "INSERT INTO entropy_scores (file_id, naming_entropy, age_spread, depth_waste, duplicate_ratio, orphan_score, composite_score, computed_at)
                     VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8)
                     ON CONFLICT(file_id) DO UPDATE SET
                        naming_entropy = excluded.naming_entropy,
                        age_spread = excluded.age_spread,
                        depth_waste = excluded.depth_waste,
                        duplicate_ratio = excluded.duplicate_ratio,
                        orphan_score = excluded.orphan_score,
                        composite_score = excluded.composite_score,
                        computed_at = excluded.computed_at",
                    params![f.id, naming, age, depth, duplicate_ratio, orphan_score, composite, now],
                )?;
                stats.files_scored += 1;
            }

            stats.dirs_scored += 1;
        }

        Ok(stats)
    }

    /// Generate suggestions based on current entropy scores.
    pub fn generate_suggestions(&self) -> Result<Vec<Suggestion>> {
        let conn = self.conn.lock().unwrap();
        let mut suggestions = Vec::new();
        let now = Utc::now().to_rfc3339();

        // Find directories with high age spread where all files are old → suggest archive
        let mut archive_stmt = conn.prepare(
            "SELECT f.path, es.age_spread, es.composite_score
             FROM entropy_scores es
             JOIN files f ON es.file_id = f.id
             WHERE es.age_spread > 0.8
             AND f.modified_at < ?1
             ORDER BY es.composite_score DESC
             LIMIT 50",
        )?;
        let six_months_ago = (Utc::now() - chrono::Duration::days(180)).timestamp();
        let archive_candidates: Vec<(String, f64)> = archive_stmt
            .query_map(params![six_months_ago], |row| {
                Ok((row.get::<_, String>(0)?, row.get::<_, f64>(2)?))
            })?
            .filter_map(|r| r.ok())
            .collect();

        if !archive_candidates.is_empty() {
            let paths: Vec<String> = archive_candidates.iter().map(|(p, _)| p.clone()).collect();
            let confidence = archive_candidates.iter().map(|(_, s)| s).sum::<f64>()
                / archive_candidates.len() as f64;
            insert_suggestion_if_new(
                &conn,
                "archive",
                &paths,
                "Files haven't been modified in 6+ months",
                confidence.min(1.0),
                None,
                &now,
            )?;
            suggestions.extend(load_pending_suggestions(&conn)?);
        }

        // Find duplicates → suggest deduplicate
        let mut dup_stmt = conn.prepare(
            "SELECT f1.path, f2.path, f1.size_bytes
             FROM file_content_hashes h1
             JOIN file_content_hashes h2 ON h1.sha256 = h2.sha256 AND h1.file_id < h2.file_id
             JOIN files f1 ON h1.file_id = f1.id
             JOIN files f2 ON h2.file_id = f2.id
             LIMIT 50",
        )?;
        let dup_pairs: Vec<(String, String, i64)> = dup_stmt
            .query_map([], |row| {
                Ok((row.get(0)?, row.get(1)?, row.get(2)?))
            })?
            .filter_map(|r| r.ok())
            .collect();

        for (path_a, path_b, size) in &dup_pairs {
            insert_suggestion_if_new(
                &conn,
                "deduplicate",
                &[path_a.clone(), path_b.clone()],
                "Exact duplicate files found",
                0.95,
                Some(*size),
                &now,
            )?;
        }

        // Load all pending suggestions
        suggestions = load_pending_suggestions(&conn)?;
        Ok(suggestions)
    }

    /// Dismiss a suggestion.
    pub fn dismiss_suggestion(&self, id: i64) -> Result<()> {
        let conn = self.conn.lock().unwrap();
        let now = Utc::now().to_rfc3339();
        conn.execute(
            "UPDATE suggestions SET status = 'dismissed', updated_at = ?1 WHERE id = ?2",
            params![now, id],
        )?;
        Ok(())
    }

    /// Defer a suggestion for N days.
    pub fn defer_suggestion(&self, id: i64, days: u32) -> Result<()> {
        let conn = self.conn.lock().unwrap();
        let now = Utc::now().to_rfc3339();
        let until = (Utc::now() + chrono::Duration::days(days as i64)).to_rfc3339();
        conn.execute(
            "UPDATE suggestions SET status = 'deferred', deferred_until = ?1, updated_at = ?2 WHERE id = ?3",
            params![until, now, id],
        )?;
        Ok(())
    }

    /// Accept a suggestion — move affected files to staging.
    pub fn accept_suggestion(&self, id: i64) -> Result<()> {
        let conn = self.conn.lock().unwrap();
        let mut stmt = conn.prepare(
            "SELECT affected_paths, suggestion_type FROM suggestions WHERE id = ?1",
        )?;
        let (paths_json, _stype): (String, String) = stmt.query_row(params![id], |row| {
            Ok((row.get(0)?, row.get(1)?))
        })?;
        let paths: Vec<String> = serde_json::from_str(&paths_json)?;
        drop(stmt);

        // Create staging directory
        let timestamp = Utc::now().format("%Y%m%d_%H%M%S").to_string();
        let staging_dir = self.staging_root.join(&timestamp);
        std::fs::create_dir_all(&staging_dir)?;

        // Move files to staging
        let mut manifest = Vec::new();
        for path_str in &paths {
            let src = Path::new(path_str);
            if src.exists() {
                let dest = staging_dir.join(src.file_name().unwrap_or_default());
                std::fs::copy(src, &dest)?;
                manifest.push(serde_json::json!({
                    "original": path_str,
                    "staged": dest.to_string_lossy(),
                }));
            }
        }

        // Write manifest
        let manifest_path = staging_dir.join("manifest.json");
        std::fs::write(&manifest_path, serde_json::to_string_pretty(&manifest)?)?;

        // Update suggestion status
        let now = Utc::now().to_rfc3339();
        conn.execute(
            "UPDATE suggestions SET status = 'accepted', updated_at = ?1 WHERE id = ?2",
            params![now, id],
        )?;

        Ok(())
    }

    /// Get summary for the UI.
    pub fn summary(&self) -> Result<EntropySummary> {
        let conn = self.conn.lock().unwrap();

        let pending: i64 = conn.query_row(
            "SELECT COUNT(*) FROM suggestions WHERE status = 'pending'",
            [],
            |row| row.get(0),
        )?;

        let space: i64 = conn
            .query_row(
                "SELECT COALESCE(SUM(space_savings_bytes), 0) FROM suggestions WHERE status = 'pending'",
                [],
                |row| row.get(0),
            )
            .unwrap_or(0);

        // Top 5 highest entropy directories
        let mut stmt = conn.prepare(
            "SELECT substr(f.path, 1, length(f.path) - length(f.name) - 1) as dir,
                    AVG(es.composite_score) as avg_score
             FROM entropy_scores es
             JOIN files f ON es.file_id = f.id
             GROUP BY dir
             ORDER BY avg_score DESC
             LIMIT 5",
        )?;
        let top_dirs: Vec<(String, f64)> = stmt
            .query_map([], |row| Ok((row.get(0)?, row.get(1)?)))?
            .filter_map(|r| r.ok())
            .collect();

        Ok(EntropySummary {
            pending_suggestions: pending as usize,
            space_reclaimable_bytes: space,
            highest_entropy_dirs: top_dirs,
        })
    }

    /// Run the idle monitoring loop. Call from a spawned task.
    pub async fn start(&self, last_interaction: Arc<std::sync::Mutex<std::time::Instant>>) {
        let idle_threshold = Duration::from_secs(self.config.idle_threshold_minutes * 60);
        let min_interval = Duration::from_secs(self.config.min_interval_minutes * 60);
        let mut last_run = std::time::Instant::now() - min_interval; // Allow immediate first run

        loop {
            if self.cancel.load(Ordering::Relaxed) {
                break;
            }
            tokio::time::sleep(Duration::from_secs(30)).await;

            let idle_duration = last_interaction.lock().unwrap().elapsed();
            let since_last_run = last_run.elapsed();

            if idle_duration >= idle_threshold && since_last_run >= min_interval {
                // Run scoring pass
                if let Ok(stats) = self.score_pass() {
                    if stats.dirs_scored > 0 {
                        let _ = self.generate_suggestions();
                    }
                }
                last_run = std::time::Instant::now();
            }
        }
    }
}

fn insert_suggestion_if_new(
    conn: &Connection,
    stype: &str,
    paths: &[String],
    reason: &str,
    confidence: f64,
    space_savings: Option<i64>,
    now: &str,
) -> Result<()> {
    let paths_json = serde_json::to_string(paths)?;

    // Check if already exists
    let exists: bool = conn
        .query_row(
            "SELECT COUNT(*) > 0 FROM suggestions WHERE suggestion_type = ?1 AND affected_paths = ?2 AND status IN ('pending', 'deferred')",
            params![stype, paths_json],
            |row| row.get(0),
        )
        .unwrap_or(false);

    if !exists {
        conn.execute(
            "INSERT INTO suggestions (suggestion_type, affected_paths, reason, confidence, space_savings_bytes, status, created_at, updated_at)
             VALUES (?1, ?2, ?3, ?4, ?5, 'pending', ?6, ?6)",
            params![stype, paths_json, reason, confidence, space_savings, now],
        )?;
    }
    Ok(())
}

fn load_pending_suggestions(conn: &Connection) -> Result<Vec<Suggestion>> {
    let mut stmt = conn.prepare(
        "SELECT id, suggestion_type, affected_paths, reason, confidence, space_savings_bytes, status
         FROM suggestions
         WHERE status = 'pending'
         ORDER BY confidence DESC",
    )?;
    let suggestions = stmt
        .query_map([], |row| {
            let paths_json: String = row.get(2)?;
            let paths: Vec<String> =
                serde_json::from_str(&paths_json).unwrap_or_default();
            Ok(Suggestion {
                id: row.get(0)?,
                suggestion_type: row.get(1)?,
                affected_paths: paths,
                reason: row.get(3)?,
                confidence: row.get(4)?,
                space_savings_bytes: row.get(5)?,
                status: row.get(6)?,
            })
        })?
        .filter_map(|r| r.ok())
        .collect();
    Ok(suggestions)
}

fn dirs_home() -> PathBuf {
    directories::BaseDirs::new()
        .map(|d| d.home_dir().to_path_buf())
        .unwrap_or_else(|| PathBuf::from("/tmp"))
}
